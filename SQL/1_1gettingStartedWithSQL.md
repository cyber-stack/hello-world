# -*- coding: utf-8 -*-
"""1_1gettingStartedWithSQL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129x719NC18xpUg6UZi_E7zlfqa9rqG0V

# **Getting started with SQL**

---

# Introduction

SQL means **Structured Query Language**. It is the programming language used with databases and it is an important skill for any data scientist. We will use **BigQuery** to build our skills in SQL. **BigQuery** is a web service that allows its users to apply SQL to huge datasets.

Thus, we first need to learn some **BigQuery** commands.
Creating a **Client object** plays a central role in retrieving information for **BigQuery** datasets.
We will work with a dataset of posts on `Hacker News`, a website focusing on computer science and cybersecurity news.

# Setting up the work environment
"""

import google
from google.oauth2 import service_account
from google.cloud import bigquery
from google.colab import auth, drive

# drive.mount('/content/drive',force_remount=True)

# authentication to BigQuery API
auth.authenticate_user()
print('Authenticated')

# getting the credentials
credentials = service_account.Credentials.from_service_account_file('drive/My Drive/Kaggle/KaggleSQL/credForBigQuery.json')

# my project id
project_id1 = 'peppy-citron-270702'

# then we create the important Client object using our project id
client = bigquery.Client(project_id1)

# then we create the important Client object using our project id
client = bigquery.Client(project_id1)

"""**Importing the dataset**

In BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project. To access the dataset, 

*   we construct a reference to the dataset with the dataset() method.
*    we use the get_dataset() method, along with the reference we just constructed, to fetch the dataset.
"""

# Construct a reference to the "hacker_news" dataset
dataset_ref = client.dataset("hacker_news", project='bigquery-public-data')

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

"""Each dataset in **relational algebra** is a collection of tables. It is like a spreadsheet file containing mulitple tables."""

# List all the tables in the "hacker_news" dataset using list_tables() method
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset using table.table_id
for table in tables :
  print (table.table_id)

"""Now we fetch the `full` table in the `hacker_news` dataset."""

# Construct a reference to the "full" table
table_ref = dataset_ref.table("full")

# API request - fetch the table
table = client.get_table(table_ref)
# it makes the access to this table easier

"""**Understanding the Client Object**

![Understanding the Client](https://drive.google.com/uc?id=1lGsdCzhplHxOc0a3fjxqzxos-JOfo7SE)

# Table schema

The structure of a table is called a **schema**. Let's investigate the `full` table we have just fetched.
"""

# Print information on all the columns in the "full" table in the "hacker_news" dataset
table.schema

"""Each `SchemaField` refers to a specific column of the `full` table (also known as a **field**). The .schema() method returns the following outputs :


*   the **name** of the column
*   the **fieldtype** in the column
*   the **mode** of the column. It specifies the authorized values in this column (example : 'NULLABLE')
*   a **description** of the data is that column

We can preview the `full` table using a pandas dataframe. This also allows us to check the reliability of the `SchemaFields`.
"""

# Preview the first five lines of the "full" table
# we use the list.rows() method and convert the output to a dataframe using .to_dataframe() method
client.list_rows(table, max_results = 5).to_dataframe()

# Preview the first five entries in the two firsts column of the "full" table
client.list_rows(table, selected_fields=table.schema[:2], max_results=5).to_dataframe()

"""# Exercice : Getting started with SQL and BigQuery

**Introduction**

We will now explore data describing crime in the city of Chicago.

**Importing the dataset**
"""

import google
from google.oauth2 import service_account
from google.cloud import bigquery
from google.colab import auth, drive

# drive.mount('/content/drive',force_remount=True)

# authentication to BigQuery API
auth.authenticate_user()
print('Authenticated')

# getting the credentials
credentials = service_account.Credentials.from_service_account_file('drive/My Drive/Kaggle/KaggleSQL/credForBigQuery.json')

# my project id
project_id1 = 'peppy-citron-270702'

# then we create the important Client object using our project id
client = bigquery.Client(project_id1)

# we first create a client object
client = bigquery.Client()

# we construct a reference for the dataset
dataset_ref = client.dataset("chicago_crime", project ='bigquery-public-data')

# we fetch the dataset
dataset = client.get_dataset(dataset_ref)

"""**Count tables in the dataset**

How many tables are in the `chicago_crime` dataset ?
"""

print("There is/are {} table/s in the dataset chicago_crime".format(len(list(client.list_tables(dataset)))))

"""**Explore the table schema**

How many columns in the `crime` table have `TIMESTAMP` data?
"""

# we create a reference for the crime table in the chicago_crime dataset
table_ref = dataset_ref.table('crime')

# we fetch the crime table in the chicago_crime dataset
table = client.get_table(table_ref)

# the attributes of a SchemaField object are description, field_type, fields, is_nullable, mode and name

c=0
for schemafield in list(table.schema) :
    if schemafield.field_type == 'TIMESTAMP' :
        c+=1
print(c)

"""**Create a crime map**

If you wanted to create a map with a dot at the location of each crime, what are the names of the two fields you likely need to pull out of the `crime` table to plot the crimes on a map?
"""

# getting the location fields
for schemafield in list(table.schema) :
    print(schemafield.name)
    print('----------')

# studying the location fields 
client.list_rows(table, selected_fields=[table.schema[15],table.schema[16],
                 table.schema[19],table.schema[20],table.schema[21]],
                max_results=10).to_dataframe()
